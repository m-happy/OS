README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should contain the following files:

 Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [cool root]/src/PA2/lextest.cc
 mycoolc         -> [cool root]/PA2/mycoolc
 stringtab.cc    -> [cool root]/PA2/stringtab.cc
 utilities.cc    -> [cool root]/PA2/utilities.cc
 handle_flags.cc -> [cool root]/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[cool root]/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. 

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% make lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% make dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	If you change architectures you must issue

	% make clean

	when you switch from one type of machine to the other.
	If at some point you get weird errors from the linker,	
	you probably forgot this step.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------

The first section above %% contains the defintions whitespaces, ints, strings, INVALID, SINGLES and other neccessory definition such as comments to have the count of nested comments, curr_lineno for current line no.
The rules start with "DARROW" defined in defination representing "<=" and return tokens for it, same goes for assignment and LE.
Keywords are previously defined in the language have special meaning in langugae than the normal strings or characters. The rule checks for an optional "i?" followed by keywords defined in the language such as, if, else, while, case, switch etc and then gives them a token.
True and false are special because they are case sensitive and when they are matched by scanned string it addes it to the table.
"ints" regular expression is defined in the definition section of the and whenever they are matched they are assigned the token, same for object and and types.
operators and special symbols are included in SINGLES and invalid characters scanned are defined in INVALID and when they are scanned the lexer returns an error message.
In comments if it first finds a "*)" then it returns an errors as it does not have any starting parentheises.
If it finds "(*" then it takes it as a start of the comment and uses a variable "comments" to count this if another "(*" comes then it increments it and if it finds "*)" then it decrements the "comments" count by one and checks if it is equal to zero if yes the comment ends here, if not then it continues, but if the count value of "comments" is negative then it throws an error saying "Unmatched *)". Inside comment if "\n" comes the line counter increments to keep and which line we are at, and "." helps to eat everything and {whitespace}+ is also eated, and if it encounter EOF inside comments then returns an error message.
String starts with '"' and then a pointer string_buf_ptr is assigned which keeps track of it, and at first it checks if the string is too long if yes then assigns null to string_buf_ptr and throws an error, if inside string it encounter EOF then also throws an error, same if it finds strings contains null character, if it didn't find terminated '"' it throws an error with string not terminated.
escape such as \n, \t , \b,\f. String contain everything except null character.
"." rule makes sure everything except these rule are eaten.
